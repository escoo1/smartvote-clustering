{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14814462",
   "metadata": {},
   "source": [
    "\n",
    "# Smartvote – Clean Clustering Pipeline  \n",
    "*(generated 2025-06-02)*\n",
    "\n",
    "Dieses Notebook erstellt saubere Themen‑Cluster aus deutschsprachigen Fragen\n",
    "und exportiert die Ergebnisse als Excel‑Datei.\n",
    "\n",
    "**Highlights gegenüber der alten Version**\n",
    "\n",
    "* nur die benötigten Spalten werden exportiert  \n",
    "* keine verschachtelten `noise_noise_…`‑Labels mehr  \n",
    "* Cluster mit weniger als 2 Fragen werden automatisch als *Noise* behandelt  \n",
    "* entfernte Duplikate werden am Ende wieder ihrem Repräsentanten‑Cluster\n",
    "  zugeordnet und mit ausgegeben  \n",
    "* klarer, modularer Code – jede Zelle macht genau einen Schritt und gibt\n",
    "  eine kurze Zusammenfassung aus  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5861e36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import hdbscan\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from nltk.corpus import stopwords\n",
    "import nltk, sys, warnings, math\n",
    "\n",
    "# ---------- Parameter ----------\n",
    "DATA_PATH              = Path('df_de_final.xlsx')       # Eingabedatei\n",
    "EXPORT_PATH            = Path('cluster_ergebnis.xlsx')  # Ausgabe‑Datei\n",
    "\n",
    "SBERT_MODEL_NAME       = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "\n",
    "# Clustering\n",
    "HDBSCAN_MIN_CLUSTER_SIZE = 2\n",
    "HDBSCAN_MIN_SAMPLES      = None        # None = auto = min_cluster_size//2\n",
    "MIN_FINAL_CLUSTER_SIZE   = 2           # alles <2 wird als Noise behandelt\n",
    "\n",
    "# Duplikaterkennung\n",
    "DUP_EPS = 0.05                         # DBSCAN‑Epsilon im TF‑IDF‑Raum\n",
    "\n",
    "# Re‑Assignment\n",
    "REASSIGN_SIM_THRESH = 0.70             # Schwelle, ab der Noise‑Fragen einem Cluster zugeordnet werden\n",
    "\n",
    "# NLTK‐Stopwörter laden (nur 1× pro Session)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "GERMAN_STOP = stopwords.words('german')\n",
    "\n",
    "print('Parameter geladen.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e324d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1 ) Daten laden & Grundreinigung\n",
    "df_all = pd.read_excel(DATA_PATH)\n",
    "print(f'Geladene Zeilen: {len(df_all):,}')\n",
    "\n",
    "df_all['frage_text_norm'] = (\n",
    "    df_all['Frage_Text'].astype(str)\n",
    "          .str.strip()\n",
    "          .str.lower()\n",
    ")\n",
    "\n",
    "print('Norm‑Spalte fertig.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd05db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2 ) Duplikaterkennung mit TF‑IDF + DBSCAN\n",
    "tfidf  = TfidfVectorizer(stop_words=GERMAN_STOP)\n",
    "X_tfidf = tfidf.fit_transform(df_all['frage_text_norm'])\n",
    "\n",
    "dbscan = DBSCAN(eps=DUP_EPS, min_samples=2, metric='cosine')\n",
    "dupe_labels = dbscan.fit_predict(X_tfidf)\n",
    "\n",
    "df_all['dupe_cluster']  = dupe_labels\n",
    "df_all['dupe_is_main']  = df_all.groupby('dupe_cluster').cumcount() == 0\n",
    "\n",
    "# --- Repräsentanten auswählen & eindeutige Fragen extrahieren ---\n",
    "unique_mask = (df_all['dupe_cluster'] == -1) | (df_all['dupe_is_main'])\n",
    "df_unique   = df_all[unique_mask].copy().reset_index(drop=True)\n",
    "\n",
    "print(f'Eindeutige Fragen nach Duplikaterkennung: {len(df_unique):,}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f376ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3 ) SBERT‑Embeddings berechnen\n",
    "model = SentenceTransformer(SBERT_MODEL_NAME)\n",
    "embeddings = model.encode(\n",
    "    df_unique['frage_text_norm'].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    convert_to_tensor=True\n",
    ")\n",
    "print('Embeddings erstellt.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23025fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4 ) HDBSCAN‑Clustering\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size      = HDBSCAN_MIN_CLUSTER_SIZE,\n",
    "    min_samples           = HDBSCAN_MIN_SAMPLES,\n",
    "    metric                = 'euclidean',\n",
    "    cluster_selection_method = 'eom'\n",
    ")\n",
    "cluster_labels = clusterer.fit_predict(embeddings.cpu().numpy())\n",
    "df_unique['cluster'] = cluster_labels\n",
    "\n",
    "print('Anzahl Cluster (exkl. Noise):',\n",
    "      len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539a9c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5 ) Ein‑Fragen‑Cluster als Noise behandeln\n",
    "size_map = df_unique.groupby('cluster').size()\n",
    "tiny_clusters = size_map[size_map < MIN_FINAL_CLUSTER_SIZE].index\n",
    "\n",
    "df_unique.loc[df_unique['cluster'].isin(tiny_clusters), 'cluster'] = -1\n",
    "print(f'Als Noise markiert (Grösse <{MIN_FINAL_CLUSTER_SIZE}): {len(tiny_clusters)} Cluster')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d796cba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6 ) Noise‑Punkte optional zuordnen\n",
    "import torch\n",
    "noise_idx = df_unique[df_unique['cluster'] == -1].index\n",
    "\n",
    "if len(noise_idx):\n",
    "    noise_emb = embeddings[noise_idx]\n",
    "    clustered_mask = df_unique['cluster'] != -1\n",
    "    clustered_emb  = embeddings[clustered_mask]\n",
    "\n",
    "    for i, idx in enumerate(noise_idx):\n",
    "        sims = util.cos_sim(noise_emb[i], clustered_emb)[0].cpu().numpy()\n",
    "        best_idx   = sims.argmax()\n",
    "        best_sim   = sims[best_idx]\n",
    "\n",
    "        if best_sim >= REASSIGN_SIM_THRESH:\n",
    "            best_cluster = df_unique.iloc[np.where(clustered_mask)[0][best_idx]]['cluster']\n",
    "            # -------- sauberes noise‑Label ohne Verdopplung -----------\n",
    "            if isinstance(best_cluster, str) and best_cluster.startswith('noise_'):\n",
    "                base = best_cluster.split('noise_')[-1]\n",
    "                df_unique.at[idx, 'cluster'] = f'noise_{base}'\n",
    "            else:\n",
    "                df_unique.at[idx, 'cluster'] = f'noise_{best_cluster}'\n",
    "\n",
    "print('Noise‑Reassignment abgeschlossen.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f059d898",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7 ) Hauptfrage & Ähnlichkeit\n",
    "df_unique['ist_hauptfrage']              = False\n",
    "df_unique['ähnlichkeit_zur_hauptfrage']  = np.nan\n",
    "\n",
    "final_emb = model.encode(df_unique['frage_text_norm'].tolist(),\n",
    "                         convert_to_tensor=True)\n",
    "\n",
    "for cl in df_unique['cluster'].unique():\n",
    "    cl_mask = df_unique['cluster'] == cl\n",
    "    idxs    = np.where(cl_mask)[0]\n",
    "    if not len(idxs):\n",
    "        continue\n",
    "    centroid = final_emb[idxs].mean(dim=0, keepdim=True)\n",
    "    sims     = util.cos_sim(centroid, final_emb[idxs])[0].cpu().numpy()\n",
    "\n",
    "    best_local_idx = idxs[sims.argmax()]\n",
    "    df_unique.at[best_local_idx, 'ist_hauptfrage'] = True\n",
    "    df_unique.loc[cl_mask, 'ähnlichkeit_zur_hauptfrage'] = sims\n",
    "\n",
    "print('Hauptfragen markiert & Ähnlichkeiten berechnet.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07589a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8 ) Cluster auf alle Duplikats‑Zeilen übertragen\n",
    "# 8a) Cluster‑Info von df_unique an df_all mergen\n",
    "df_all = df_all.merge(\n",
    "    df_unique[['ID_gesamt', 'cluster']],\n",
    "    on='ID_gesamt',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 8b) Fehlende Cluster über Repräsentanten‑Map auffüllen\n",
    "rep_map = (df_all[df_all['dupe_is_main']]\n",
    "           .set_index('dupe_cluster')['cluster'])\n",
    "\n",
    "def fill_cluster(row):\n",
    "    if pd.isna(row['cluster']):\n",
    "        return rep_map.get(row['dupe_cluster'], -1)\n",
    "    return row['cluster']\n",
    "\n",
    "df_all['cluster'] = df_all.apply(fill_cluster, axis=1)\n",
    "\n",
    "print('Cluster auf alle Zeilen übertragen.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19c2a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 9 ) Finales DataFrame + Export\n",
    "export_cols = ['cluster',\n",
    "               'ist_hauptfrage',\n",
    "               'ähnlichkeit_zur_hauptfrage',\n",
    "               'Frage_Text',\n",
    "               'ID_gesamt']\n",
    "\n",
    "# Flag & Ähnlichkeit für Nicht‑Repräsentanten ergänzen (optional)\n",
    "# -> Duplikate erhalten dieselbe Ähnlichkeit wie ihr Repräsentant\n",
    "sim_map = (df_unique\n",
    "           .set_index('ID_gesamt')['ähnlichkeit_zur_hauptfrage'])\n",
    "df_all['ähnlichkeit_zur_hauptfrage'] = df_all['ID_gesamt'].map(sim_map)\n",
    "\n",
    "# Hauptfrage‑Flag für Duplikate = False\n",
    "df_all['ist_hauptfrage'] = df_all['ID_gesamt'].isin(\n",
    "    df_unique[df_unique['ist_hauptfrage']]['ID_gesamt']\n",
    ")\n",
    "\n",
    "df_export = df_all[export_cols].copy()\n",
    "\n",
    "# sinnvolle Sortierung\n",
    "cluster_sizes = df_export.groupby('cluster').size().sort_values(ascending=False)\n",
    "df_export['cluster'] = pd.Categorical(df_export['cluster'],\n",
    "                                      categories=cluster_sizes.index,\n",
    "                                      ordered=True)\n",
    "df_export.sort_values(['cluster', 'ist_hauptfrage',\n",
    "                       'ähnlichkeit_zur_hauptfrage'],\n",
    "                      ascending=[True, False, False],\n",
    "                      inplace=True)\n",
    "\n",
    "# --- Excel ---\n",
    "df_export.to_excel(EXPORT_PATH, index=False)\n",
    "print(f'✓ Export fertig: {EXPORT_PATH}')\n",
    "print('\\nVorschau:')\n",
    "display(df_export.head())\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
