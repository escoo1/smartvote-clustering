{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8258325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n## 0 | Setup & Parameter\\n- Passe `DATA_PATH` bei Bedarf an.\\n- `DUP_EPS` bestimmt, ab welcher Cosine‐Distanz zwei Fragen als quasi identisch gelten.\\n- `MIN_CLUSTER_SIZE` fuer HDBSCAN kannst du hier zentral steuern.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Smartvote – Clean Clustering Pipeline\n",
    "# =====================================\n",
    "# Dieses Notebook fuehrt alle Schritte in logisch getrennten Zellen aus:\n",
    "# 0) Parameter & Setup\n",
    "# 1) Daten laden + Normalisierung\n",
    "# 2) Deduplication via TF‑IDF + DBSCAN\n",
    "# 3) SBERT‑Embeddings & HDBSCAN‑Clustering (nur Repräsentanten)\n",
    "# 4) Mapping: Duplikate erben das Cluster ihres Repräsentanten\n",
    "# 5) Kennzahlen & Cluster‑Analyse\n",
    "# 6) Export in eine Excel‑Datei\n",
    "#\n",
    "# Abhaengigkeiten (nur allgemein verfuegbare Pakete):\n",
    "# pandas, numpy, scikit‑learn, sentence‑transformers, hdbscan, openpyxl\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## 0 | Setup & Parameter\n",
    "- Passe `DATA_PATH` bei Bedarf an.\n",
    "- `DUP_EPS` bestimmt, ab welcher Cosine‐Distanz zwei Fragen als quasi identisch gelten.\n",
    "- `MIN_CLUSTER_SIZE` fuer HDBSCAN kannst du hier zentral steuern.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "077471b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter geladen.\n"
     ]
    }
   ],
   "source": [
    "# 0 | Setup & Parameter\n",
    "from pathlib import Path\n",
    "import unicodedata, re, json, itertools\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import hdbscan\n",
    "\n",
    "DATA_PATH        = Path(\"df_de_final.xlsx\")          # Rohdaten\n",
    "EXPORT_PATH      = Path(\"cluster_ergebnis.xlsx\")      # Output‑Excel\n",
    "DUP_EPS          = 0.05     # 1 – 0.95 Ähnlichkeit => Cosine‑Distanz 0.05\n",
    "MIN_CLUSTER_SIZE = 2        # fuer HDBSCAN\n",
    "\n",
    "print(\"Parameter geladen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f3bbfd",
   "metadata": {},
   "source": [
    "## 1 | Daten importieren & normalisieren\n",
    "Die Funktion `norm()` vereinheitlicht Gross-/Kleinschreibung,\n",
    "Satzzeichen und Unicode‐Varianten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cf3b3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eingelesene Zeilen: 7459\n",
      "Normalisiere Texte …\n",
      "Fertige Normalisierung.\n",
      "Anzahl eindeutiger normierter Texte: 3116\n"
     ]
    }
   ],
   "source": [
    "# 1.1 Daten laden\n",
    "if DATA_PATH.suffix.lower() == \".csv\":\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "else:\n",
    "    df = pd.read_excel(DATA_PATH)\n",
    "print(\"Eingelesene Zeilen:\", len(df))\n",
    "\n",
    "# 1.2 Normalisierung\n",
    "_punct = re.compile(r\"[^\\w\\s]\", re.UNICODE)\n",
    "\n",
    "def norm(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    t = unicodedata.normalize(\"NFKC\", text)\n",
    "    t = \" \".join(t.lower().strip().split())\n",
    "    t = (\n",
    "        t.replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "         .replace(\"‚\", \"'\").replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "    )\n",
    "    t = _punct.sub(\"\", t)\n",
    "    return \" \".join(t.split())\n",
    "\n",
    "print(\"Normalisiere Texte …\")\n",
    "df[\"text_norm\"] = df[\"Frage_Text\"].apply(norm)\n",
    "print(\"Fertige Normalisierung.\")\n",
    "\n",
    "# Kennzahl 1\n",
    "print(\"Anzahl eindeutiger normierter Texte:\", df[\"text_norm\"].nunique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4d98a5",
   "metadata": {},
   "source": [
    "## 2 | Deduplication\n",
    "Wir verwenden TF‑IDF + DBSCAN, um identische und nahezu identische Fragen\n",
    "zusammenzufassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7fcc116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baue TF‑IDF Matrix …\n",
      "Finde Duplikatsgruppen …\n",
      "Repräsentanten: 2940 | Duplikate: 4519\n",
      "Duplikatsgruppen gesamt: 2940\n"
     ]
    }
   ],
   "source": [
    "# 2.1 TF‑IDF Matrix\n",
    "print(\"Baue TF‑IDF Matrix …\")\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "X_tfidf    = vectorizer.fit_transform(df[\"text_norm\"])\n",
    "\n",
    "# 2.2 DBSCAN in Cosine‑Distanz (Radius = DUP_EPS)\n",
    "print(\"Finde Duplikatsgruppen …\")\n",
    "db_dup = DBSCAN(eps=DUP_EPS, min_samples=1, metric=\"cosine\")\n",
    "df[\"dup_grp\"] = db_dup.fit_predict(X_tfidf)\n",
    "\n",
    "# 2.3 Repräsentant: erste Frage einer Gruppe\n",
    "first_idx           = df.groupby(\"dup_grp\").head(1).index\n",
    "df[\"is_rep\"]        = df.index.isin(first_idx)\n",
    "rep_df              = df[df[\"is_rep\"]].copy()\n",
    "print(\"Repräsentanten:\", len(rep_df), \"| Duplikate:\", (~df[\"is_rep\"]).sum())\n",
    "\n",
    "# Kennzahl 2\n",
    "print(\"Duplikatsgruppen gesamt:\", df[\"dup_grp\"].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2edaab7",
   "metadata": {},
   "source": [
    "## 3 | Embeddings & HDBSCAN auf den Repräsentanten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1de3943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berechne SBERT Embeddings (nur Repräsentanten) …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced3d9c98590459a99aab3bb9257b88f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuehre HDBSCAN durch …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\olive\\git\\EPRP\\.conda\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\olive\\git\\EPRP\\.conda\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 209 | Noise: 1890\n"
     ]
    }
   ],
   "source": [
    "print(\"Berechne SBERT Embeddings (nur Repräsentanten) …\")\n",
    "model      = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "rep_df[\"embedding\"] = list(model.encode(rep_df[\"text_norm\"].tolist(), show_progress_bar=True))\n",
    "\n",
    "# 3.1 HDBSCAN\n",
    "print(\"Fuehre HDBSCAN durch …\")\n",
    "hdb = hdbscan.HDBSCAN(min_cluster_size=MIN_CLUSTER_SIZE, metric=\"euclidean\")\n",
    "rep_df[\"cluster\"] = hdb.fit_predict(np.vstack(rep_df[\"embedding\"]))\n",
    "\n",
    "# Kennzahl 3\n",
    "n_clusters = rep_df[\"cluster\"].nunique() - (rep_df[\"cluster\"] == -1).any()\n",
    "noise_cnt  = (rep_df[\"cluster\"] == -1).sum()\n",
    "print(\"Cluster:\", n_clusters, \"| Noise:\", noise_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daeca23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise-Fragen exportiert: 1890 → noise_questions.pkl\n",
      "Clusterzentren gespeichert: 209 → cluster_centers.pkl\n",
      "Repräsentanten gespeichert: 2940 Zeilen → rep_df.pkl\n"
     ]
    }
   ],
   "source": [
    "# 3.2 | Noise-Fragen exportieren\n",
    "noise_df = rep_df[rep_df[\"cluster\"] == -1].copy()\n",
    "noise_df.to_pickle(\"noise_questions.pkl\")\n",
    "print(f\"Noise-Fragen exportiert: {len(noise_df)} → noise_questions.pkl\")\n",
    "\n",
    "# 3.3 | Clusterzentren exportieren (für Reassignment)\n",
    "import joblib, numpy as np\n",
    "centers = (rep_df[rep_df[\"cluster\"] != -1]\n",
    "           .groupby(\"cluster\")[\"embedding\"]\n",
    "           .apply(lambda x: np.mean(np.vstack(x), axis=0)))\n",
    "joblib.dump(centers, \"cluster_centers.pkl\")\n",
    "print(f\"Clusterzentren gespeichert: {len(centers)} → cluster_centers.pkl\")\n",
    "\n",
    "# 3.4 | komplettes DataFrame exportieren\n",
    "rep_df.to_pickle(\"rep_df.pkl\")     # nur die Repräsentanten\n",
    "print(\"Repräsentanten gespeichert:\", len(rep_df), \"Zeilen → rep_df.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764e83c6",
   "metadata": {},
   "source": [
    "## 4 | Mapping: Duplikate erben das Cluster\n",
    "- Fuege Embeddings fuer alle Fragen hinzu (fuer Similarity‑Score).\n",
    "- Weise jedem Duplikat das Cluster seines Repräsentanten zu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e4d32f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings fuer *alle* Fragen berechnen (1‑mal, kann dauern) …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3870b4e061f4ad899b68d6e457b462e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berechne Similarity …\n",
      "Unzugeordnet (Cluster = NaN): 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Embeddings fuer *alle* Fragen berechnen (1‑mal, kann dauern) …\")\n",
    "df[\"embedding\"] = list(model.encode(df[\"text_norm\"].tolist(), show_progress_bar=True))\n",
    "\n",
    "# 4.1 Cluster vom Repräsentanten auf ganze Gruppe abbilden\n",
    "grp2cluster = rep_df.set_index(\"dup_grp\")[\"cluster\"].to_dict()\n",
    "df[\"cluster_final\"] = df[\"dup_grp\"].map(grp2cluster)\n",
    "\n",
    "# 4.2 Cosine‑Similarity jedes Satzes zu seinem Repräsentanten berechnen\n",
    "print(\"Berechne Similarity …\")\n",
    "# Vorbereitung: Embedding Lookup nach dup_grp (Root‑Embedding)\n",
    "root_emb = rep_df.set_index(\"dup_grp\")[\"embedding\"].to_dict()\n",
    "\n",
    "def cos_sim(u,v):\n",
    "    u = np.asarray(u); v = np.asarray(v)\n",
    "    return float(np.dot(u,v) / (np.linalg.norm(u)*np.linalg.norm(v) + 1e-9))\n",
    "\n",
    "df[\"sim_to_root\"] = [cos_sim(e, root_emb[g]) for e, g in zip(df[\"embedding\"], df[\"dup_grp\"])]\n",
    "\n",
    "# Kennzahl 4\n",
    "print(\"Unzugeordnet (Cluster = NaN):\", df[\"cluster_final\"].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c0e26e",
   "metadata": {},
   "source": [
    "## 5 | Analyse: Cluster‑Statistik & Top‑Woerter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36e2667d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustergrössen (Top 5):\n",
      "   cluster     n\n",
      "0       -1  3951\n",
      "1        7   131\n",
      "2      118   111\n",
      "3       61   107\n",
      "4       39    99\n",
      "\n",
      "Bestimme Top-Wörter je Cluster …\n",
      "\n",
      "Beispiel Top-Wörter:\n",
      "Cluster 0: schweizer, neuer, kampfflugzeuge, beschaffung, armee\n",
      "Cluster 1: wohlfahrt, sozialversicherungen, soziale, fürsorge\n",
      "Cluster 2: öffentliche, sicherheit, ordnung, verteidigung\n",
      "Cluster 3: verkehr, strassenverkehr, öffentlicher\n",
      "Cluster 4: nahrungsmittel, faire, eingehalten, dass, arbeitsbedingungen\n"
     ]
    }
   ],
   "source": [
    "# 5 | Clusterstatistik & Top-Wörter\n",
    "\n",
    "# 5.1 Clustergrössen -----------------------------------------------\n",
    "size_tbl = (\n",
    "    df[\"cluster_final\"]\n",
    "    .value_counts()\n",
    "    .rename_axis(\"cluster\")\n",
    "    .reset_index(name=\"n\")\n",
    ")\n",
    "print(\"Clustergrössen (Top 5):\")\n",
    "print(size_tbl.head())\n",
    "\n",
    "# 5.2 Top-Wörter je Cluster ----------------------------------------\n",
    "print(\"\\nBestimme Top-Wörter je Cluster …\")\n",
    "\n",
    "# einfache deutschsprachige Stop-Words (frei erweiterbar)\n",
    "german_stop = [\n",
    "    \"und\", \"oder\", \"der\", \"die\", \"das\", \"ein\", \"eine\", \"einer\", \"eines\",\n",
    "    \"den\", \"im\", \"mit\", \"von\", \"für\", \"ist\", \"sind\", \"werden\", \"nicht\",\n",
    "    \"auf\", \"in\", \"an\", \"am\", \"als\", \"bei\", \"sollen\", \"sie\", \"befürworten\"\n",
    "]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "key_terms = {}\n",
    "\n",
    "for cl, grp in rep_df.groupby(\"cluster\"):\n",
    "    if cl == -1:          # Noise überspringen\n",
    "        continue\n",
    "\n",
    "    # Vectorizer OHNE 'german'-Shortcut -> eigene Liste\n",
    "    vec = TfidfVectorizer(max_features=5, stop_words=german_stop)\n",
    "    X   = vec.fit_transform(grp[\"text_norm\"])\n",
    "\n",
    "    # TF-IDF summieren -> wichtigste Wörter\n",
    "    totals = np.asarray(X.sum(axis=0)).ravel()\n",
    "    top_idx = totals.argsort()[::-1][:5]\n",
    "    vocab   = vec.get_feature_names_out()\n",
    "    key_terms[cl] = [vocab[i] for i in top_idx]\n",
    "\n",
    "print(\"\\nBeispiel Top-Wörter:\")\n",
    "for cl, kws in list(key_terms.items())[:5]:\n",
    "    print(f\"Cluster {cl}: {', '.join(kws)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e92e1df",
   "metadata": {},
   "source": [
    "## 6 | Export nach Excel\n",
    "- Sheet **Clusters**: alle Fragen, nach Cluster‑Groesse sortiert; Repräsentant steht zuerst (Similarity = 1).\n",
    "- Sheet **Noise**: alle Fragen mit Cluster = -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "548819dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spalten gefüllt – bereit für den Export.\n"
     ]
    }
   ],
   "source": [
    "# 6.a | Hauptfrage & Ähnlichkeit pro Cluster ermitteln\n",
    "\n",
    "# 6.a -----------------------------------------------------------------\n",
    "import numpy as np\n",
    "\n",
    "def cosine(u, v):\n",
    "    u = np.asarray(u); v = np.asarray(v)\n",
    "    return float(np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v) + 1e-9))\n",
    "\n",
    "# 1. Hauptfrage (= erste Zeile mit is_rep==True) je Cluster\n",
    "cluster_root_idx = {}\n",
    "for cl, grp in df.groupby(\"cluster_final\"):\n",
    "    if pd.isna(cl) or cl == -1:        # Noise / unzugeordnet\n",
    "        continue\n",
    "    root_rows = grp[grp[\"is_rep\"]]\n",
    "    if root_rows.empty:\n",
    "        root_idx = grp.index[0]        # fallback\n",
    "    else:\n",
    "        root_idx = root_rows.index[0]\n",
    "    cluster_root_idx[cl] = root_idx\n",
    "\n",
    "# 2. Spalten anlegen & füllen\n",
    "df[\"ist_hauptfrage\"]              = False\n",
    "df[\"ähnlichkeit_zur_hauptfrage\"]  = np.nan\n",
    "\n",
    "for cl, root_idx in cluster_root_idx.items():\n",
    "    root_emb  = df.at[root_idx, \"embedding\"]\n",
    "    root_text = df.at[root_idx, \"Frage_Text\"]\n",
    "\n",
    "    idxs = df.index[df[\"cluster_final\"] == cl]\n",
    "    df.loc[idxs, \"ähnlichkeit_zur_hauptfrage\"] = [\n",
    "        cosine(df.at[i, \"embedding\"], root_emb) for i in idxs\n",
    "    ]\n",
    "    df.at[root_idx, \"ist_hauptfrage\"] = True        # Flag setzen\n",
    "    df.at[root_idx, \"ähnlichkeit_zur_hauptfrage\"] = 1.0\n",
    "\n",
    "# 3. Innerhalb jedes Clusters sortieren\n",
    "df.sort_values(\n",
    "    by=[\n",
    "        \"cluster_final\",\n",
    "        \"ist_hauptfrage\",                # True (=1) zuerst => absteigend sortieren\n",
    "        \"ähnlichkeit_zur_hauptfrage\"\n",
    "    ],\n",
    "    ascending=[True, False, False],\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "print(\"Spalten gefüllt – bereit für den Export.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95de6ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erzeuge strukturierte Excel-Datei …\n",
      "Export-Spalten: ['cluster_final', 'ist_hauptfrage', 'ähnlichkeit_zur_hauptfrage', 'Frage_Text', 'ID_gesamt']\n",
      "Excel gespeichert unter: cluster_ergebnis.xlsx\n"
     ]
    }
   ],
   "source": [
    "# 6.b | Strukturierter Excel-Export\n",
    "\n",
    "# %% 6.b – Export --------------------------------------------------------\n",
    "import re\n",
    "from collections import Counter\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill\n",
    "\n",
    "print(\"Erzeuge strukturierte Excel-Datei …\")\n",
    "\n",
    "# ------------------------------------------------------------------ 6.1\n",
    "export_cols = [\n",
    "    \"cluster_final\",\n",
    "    \"ist_hauptfrage\",\n",
    "    \"ähnlichkeit_zur_hauptfrage\",\n",
    "    \"Frage_Text\",\n",
    "    \"ID_gesamt\",\n",
    "]\n",
    "\n",
    "# EINMALIGE Kopie + Altlasten entfernen\n",
    "df_export = df.copy()\n",
    "for old in (\"is_rep\", \"sim_to_root\"):\n",
    "    if old in df_export.columns:\n",
    "        df_export.drop(columns=[old], inplace=True)\n",
    "\n",
    "# prüfen, ob Schritt 5.b lief\n",
    "required = {\"ist_hauptfrage\", \"ähnlichkeit_zur_hauptfrage\"}\n",
    "missing  = required - set(df_export.columns)\n",
    "assert not missing, f\"Fehlende Spalten {missing} – erst Schritt 5.b ausführen!\"\n",
    "\n",
    "print(\"Export-Spalten:\", export_cols)\n",
    "\n",
    "# Hilfsfunktion: Top-10-Nomen (Großschreib-Heuristik)\n",
    "def extract_top_nouns(texts, top_n=10):\n",
    "    tokens = re.findall(r\"\\b[A-ZÄÖÜ][a-zäöü]+\\b\", \" \".join(map(str, texts)))\n",
    "    return \", \".join(f\"{w} ({c})\" for w, c in Counter(tokens).most_common(top_n))\n",
    "\n",
    "# ------------------------------------------------------------------ 6.2\n",
    "unzugeordnet   = df_export[df_export[\"cluster_final\"].isna()]\n",
    "noise_clusters = df_export[df_export[\"cluster_final\"] == -1]\n",
    "regular_clusters = df_export[\n",
    "    df_export[\"cluster_final\"].notna() & (df_export[\"cluster_final\"] != -1)\n",
    "]\n",
    "\n",
    "regular_grouped = sorted(\n",
    "    regular_clusters.groupby(\"cluster_final\"), key=lambda kv: len(kv[1]), reverse=True\n",
    ")\n",
    "noise_grouped = sorted(\n",
    "    noise_clusters.groupby(\"cluster_final\"), key=lambda kv: len(kv[1]), reverse=True\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------ 6.3\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.title = \"Smartvote Cluster\"\n",
    "\n",
    "bold_font   = Font(bold=True)\n",
    "yellow_fill = PatternFill(\"solid\", fgColor=\"FFFF99\")\n",
    "\n",
    "# ------------------------------------------------------------------ 6.4\n",
    "def write_cluster_section(title: str, df_cluster: pd.DataFrame, start_row: int) -> int:\n",
    "    ws.cell(start_row, 1, title).font = bold_font\n",
    "    start_row += 1\n",
    "\n",
    "    ws.cell(start_row, 1, f\"Top 10 Nomen: {extract_top_nouns(df_cluster['Frage_Text'])}\")\n",
    "    start_row += 1\n",
    "\n",
    "    # Hauptfrage zuerst, Rest nach Similarity ↓\n",
    "    df_sorted = pd.concat(\n",
    "        [\n",
    "            df_cluster[df_cluster[\"ist_hauptfrage\"]],\n",
    "            df_cluster[~df_cluster[\"ist_hauptfrage\"]].sort_values(\n",
    "                \"ähnlichkeit_zur_hauptfrage\", ascending=False\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Header\n",
    "    for col_idx, col in enumerate(export_cols, 1):\n",
    "        ws.cell(start_row, col_idx, col).font = bold_font\n",
    "    start_row += 1\n",
    "\n",
    "    # Daten\n",
    "    for _, r in df_sorted.iterrows():\n",
    "        for col_idx, col in enumerate(export_cols, 1):\n",
    "            val = r[col]\n",
    "            cell = ws.cell(start_row, col_idx, val)\n",
    "            if (\n",
    "                col == \"ähnlichkeit_zur_hauptfrage\"\n",
    "                and not r[\"ist_hauptfrage\"]\n",
    "                and val < 1.0\n",
    "            ):\n",
    "                # ganze Zeile gelb\n",
    "                for j in range(1, len(export_cols) + 1):\n",
    "                    ws.cell(start_row, j).fill = yellow_fill\n",
    "        start_row += 1\n",
    "\n",
    "    return start_row + 1  # Leerzeile\n",
    "\n",
    "# ------------------------------------------------------------------ 6.5\n",
    "row = 1\n",
    "row = write_cluster_section(\"Unzugeordnete Fragen\", unzugeordnet, row)\n",
    "for cname, grp in regular_grouped:\n",
    "    row = write_cluster_section(f\"Cluster {cname}\", grp, row)\n",
    "for cname, grp in noise_grouped:\n",
    "    row = write_cluster_section(cname, grp, row)\n",
    "\n",
    "# ------------------------------------------------------------------ 6.6\n",
    "wb.save(EXPORT_PATH)\n",
    "print(\"Excel gespeichert unter:\", EXPORT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b6ebf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vollständiges DF gespeichert: 7459 Zeilen → de_final.pkl\n"
     ]
    }
   ],
   "source": [
    "df.to_pickle(\"de_final.pkl\")       # jetzt mit cluster_final, sim_to_root, ...\n",
    "print(\"Vollständiges DF gespeichert:\", len(df), \"Zeilen → de_final.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
